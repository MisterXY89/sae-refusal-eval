{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c9baf7-5daa-48c8-b172-6b148eec5653",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2876610e-3d52-4a7b-bf0a-86fb6d822a91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wandb MA-sae-train: 03ca085d27243e9a7876d511e0c7402d861df34c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b20880-5f06-467b-bada-ac489d72f9f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sae_lens import (\n",
    "    SAETrainingRunner, LanguageModelSAERunnerConfig,\n",
    "    upload_saes_to_huggingface\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db3cab4b-8f01-4104-9b44-ef58c723fa6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ——— Toggle TEST_MODE ——————————————————————————————————————————————————\n",
    "# If True → tiny run, no logging, no uploads. If False → full run + HuggingFace upload.\n",
    "# Full run should be done via slurm job!\n",
    "TEST_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ae3470-1e93-40e5-a34b-80f481274fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# —— Schedule & batch sizing —————————————————————————————————————\n",
    "if TEST_MODE:\n",
    "    total_steps        = 10\n",
    "    batch_size_tokens  = 512\n",
    "    dataset_path       = \"karpathy/tiny_shakespeare\"\n",
    "    log_to_wandb       = False\n",
    "    wandb_project      = None\n",
    "    n_checkpoints      = 0\n",
    "    checkpoint_path    = None\n",
    "\n",
    "    # disable all precision tricks to avoid dtype mismatches\n",
    "    use_autocast       = False\n",
    "    use_autocast_lm    = False\n",
    "    use_compile_sae    = False\n",
    "    use_compile_llm    = False\n",
    "    sae_dtype          = \"float32\"\n",
    "else:\n",
    "    total_steps        = 75_000\n",
    "    batch_size_tokens  = 4_096\n",
    "    # dataset_path       = \"wikitext/wikitext-103-raw-v1\"\n",
    "    dataset_path  = \"apollo-research/monology-pile-uncopyrighted-tokenizer-EleutherAI-gpt-neox-20b\"\n",
    "    log_to_wandb       = True\n",
    "    wandb_project      = \"MA-sae-train\"\n",
    "    n_checkpoints      = 5\n",
    "    checkpoint_path    = \"./checkpoints\"\n",
    "\n",
    "    use_autocast       = True\n",
    "    use_autocast_lm    = True\n",
    "    use_compile_sae    = True\n",
    "    use_compile_llm    = True\n",
    "    sae_dtype          = \"float16\"  # or \"bfloat16\" if you want to match the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d409e9e-c252-4ad1-a039-83541831d820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_training_tokens = total_steps * batch_size_tokens\n",
    "lr_warmup_steps       = int(0.10 * total_steps)\n",
    "lr_decay_steps        = total_steps\n",
    "l1_warmup_steps       = int(0.20 * total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2626590-78ec-44dc-8417-be15ab531a17",
   "metadata": {},
   "source": [
    "1) Using `wikitext-103` for the full run: is its domain coverage sufficient,\n",
    "   or would a subset of the Pile/OpenWebText better capture diverse contexts? - \n",
    "--> !Check\n",
    "2) Expansion factor=16 on small data can kill features. For a quick run,\n",
    "   we might try 8 or even 4 to see denser utilization.\n",
    "3) dtype=\"float16\" + compile_sae can speed up, but have you compared\n",
    "   reconstruction quality vs float32?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2585ce3-eb41-44bf-a5d3-302a4a8a6610",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # — data & model hooks —\n",
    "    model_name                    = \"EleutherAI/pythia-410m-deduped\",\n",
    "    hook_name                     = \"blocks.4.hook_mlp_out\",\n",
    "    hook_layer                    = 4,\n",
    "    d_in                          = 1024,\n",
    "    dataset_path                  = dataset_path,\n",
    "    is_dataset_tokenized          = True,#False,\n",
    "    streaming                     = True,\n",
    "\n",
    "    # — SAE architecture & sparsity —\n",
    "    architecture                  = \"standard\",\n",
    "    expansion_factor              = 16,\n",
    "    l1_coefficient                = 2.0,\n",
    "    l1_warm_up_steps              = l1_warmup_steps,\n",
    "    normalize_activations         = \"expected_average_only_in\",\n",
    "    mse_loss_normalization        = \"layer\",\n",
    "\n",
    "    # — init & symmetry —\n",
    "    b_dec_init_method             = \"zeros\",\n",
    "    init_encoder_as_decoder_transpose = True,\n",
    "    decoder_heuristic_init        = False,\n",
    "\n",
    "    # — optimization & scheduling —\n",
    "    lr                            = 5e-5,\n",
    "    adam_beta1                    = 0.9,\n",
    "    adam_beta2                    = 0.999,\n",
    "    lr_scheduler_name             = \"cosineannealing\",\n",
    "    lr_warm_up_steps              = lr_warmup_steps,\n",
    "    lr_decay_steps                = lr_decay_steps,\n",
    "\n",
    "    # — context & batch sizing —\n",
    "    context_size                  = 2048, #512,\n",
    "    train_batch_size_tokens       = batch_size_tokens,\n",
    "\n",
    "    # — logging, checkpoints & precision —\n",
    "    training_tokens               = total_training_tokens,\n",
    "    feature_sampling_window       = 1_000,\n",
    "    log_to_wandb                  = log_to_wandb,\n",
    "    wandb_project                 = wandb_project,\n",
    "    wandb_log_frequency           = 100,\n",
    "    n_checkpoints                 = n_checkpoints,\n",
    "    checkpoint_path               = checkpoint_path,\n",
    "    compile_sae                   = use_compile_sae,\n",
    "    compile_llm                   = use_compile_llm,\n",
    "    autocast                      = use_autocast,\n",
    "    autocast_lm                   = use_autocast_lm,\n",
    "    device                        = \"cuda:0\",\n",
    "    seed                          = 42,\n",
    "    dtype                         = sae_dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "283ce839-5bbc-4606-9a4f-785d41750876",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL_RUN ➞ steps=75000, dataset=apollo-research/monology-pile-uncopyrighted-tokenizer-EleutherAI-gpt-neox-20b\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'TEST_MODE' if TEST_MODE else 'FULL_RUN'} ➞ steps={total_steps}, dataset={dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa8ec717-f735-406c-8959-f38137ab870a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-410m-deduped into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dacf6d13c74245a992edff2376d12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/303 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a51b19ba18483aaa09885568a62b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223b6364f54b4100aa6d0c029ee5116d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtilmankerl\u001b[0m (\u001b[33mtilmankerl-technical-university-of-vienna\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tilman.kerl/mech-interp/src/train/wandb/run-20250603_194035-mfub3wv4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tilmankerl-technical-university-of-vienna/MA-sae-train/runs/mfub3wv4' target=\"_blank\">16384-L1-2.0-LR-5e-05-Tokens-3.072e+08</a></strong> to <a href='https://wandb.ai/tilmankerl-technical-university-of-vienna/MA-sae-train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tilmankerl-technical-university-of-vienna/MA-sae-train' target=\"_blank\">https://wandb.ai/tilmankerl-technical-university-of-vienna/MA-sae-train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tilmankerl-technical-university-of-vienna/MA-sae-train/runs/mfub3wv4' target=\"_blank\">https://wandb.ai/tilmankerl-technical-university-of-vienna/MA-sae-train/runs/mfub3wv4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SAE:   0%|          | 0/307200000 [00:00<?, ?it/s]\n",
      "Estimating norm scaling factor:   0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Refilling buffer:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Refilling buffer:  10%|█         | 1/10 [00:12<01:49, 12.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "Refilling buffer:  20%|██        | 2/10 [00:12<00:43,  5.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "Refilling buffer:  30%|███       | 3/10 [00:13<00:23,  3.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "Refilling buffer:  40%|████      | 4/10 [00:14<00:13,  2.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "Refilling buffer:  50%|█████     | 5/10 [00:15<00:08,  1.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "Refilling buffer:  60%|██████    | 6/10 [00:15<00:05,  1.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "Refilling buffer:  70%|███████   | 7/10 [00:16<00:03,  1.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "Refilling buffer:  80%|████████  | 8/10 [00:17<00:02,  1.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "Refilling buffer:  90%|█████████ | 9/10 [00:18<00:00,  1.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Refilling buffer: 100%|██████████| 10/10 [00:18<00:00,  1.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                 \u001b[A\u001b[A\n",
      "\n",
      "Refilling buffer:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Estimating norm scaling factor:   0%|          | 0/1000 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 39.50 GiB of which 6.28 GiB is free. Including non-PyTorch memory, this process has 33.21 GiB memory in use. Of the allocated memory 27.81 GiB is allocated by PyTorch, and 4.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# — run training —\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sparse_autoencoder = \u001b[43mSAETrainingRunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/sae_training_runner.py:112\u001b[39m, in \u001b[36mSAETrainingRunner.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    103\u001b[39m trainer = SAETrainer(\n\u001b[32m    104\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    105\u001b[39m     sae=\u001b[38;5;28mself\u001b[39m.sae,\n\u001b[32m   (...)\u001b[39m\u001b[32m    108\u001b[39m     cfg=\u001b[38;5;28mself\u001b[39m.cfg,\n\u001b[32m    109\u001b[39m )\n\u001b[32m    111\u001b[39m \u001b[38;5;28mself\u001b[39m._compile_if_needed()\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m sae = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_trainer_with_interruption_handling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.log_to_wandb:\n\u001b[32m    115\u001b[39m     wandb.finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/sae_training_runner.py:151\u001b[39m, in \u001b[36mSAETrainingRunner.run_trainer_with_interruption_handling\u001b[39m\u001b[34m(self, trainer)\u001b[39m\n\u001b[32m    148\u001b[39m     signal.signal(signal.SIGTERM, interrupt_callback)\n\u001b[32m    150\u001b[39m     \u001b[38;5;66;03m# train SAE\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     sae = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, InterruptedException):\n\u001b[32m    154\u001b[39m     logger.warning(\u001b[33m\"\u001b[39m\u001b[33minterrupted, saving progress\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/sae_trainer.py:177\u001b[39m, in \u001b[36mSAETrainer.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> TrainingSAE:\n\u001b[32m    175\u001b[39m     pbar = tqdm(total=\u001b[38;5;28mself\u001b[39m.cfg.total_training_tokens, desc=\u001b[33m\"\u001b[39m\u001b[33mTraining SAE\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactivations_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_norm_scaling_factor_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# Train loop\u001b[39;00m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_training_tokens < \u001b[38;5;28mself\u001b[39m.cfg.total_training_tokens:\n\u001b[32m    181\u001b[39m         \u001b[38;5;66;03m# Do a training step.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/activations_store.py:424\u001b[39m, in \u001b[36mActivationsStore.set_norm_scaling_factor_if_needed\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_norm_scaling_factor_if_needed\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    420\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    421\u001b[39m         \u001b[38;5;28mself\u001b[39m.normalize_activations == \u001b[33m\"\u001b[39m\u001b[33mexpected_average_only_in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    422\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.estimated_norm_scaling_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    423\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m         \u001b[38;5;28mself\u001b[39m.estimated_norm_scaling_factor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimate_norm_scaling_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/activations_store.py:451\u001b[39m, in \u001b[36mActivationsStore.estimate_norm_scaling_factor\u001b[39m\u001b[34m(self, n_batches_for_norm_estimate)\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[32m    447\u001b[39m     \u001b[38;5;28mrange\u001b[39m(n_batches_for_norm_estimate), desc=\u001b[33m\"\u001b[39m\u001b[33mEstimating norm scaling factor\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    448\u001b[39m ):\n\u001b[32m    449\u001b[39m     \u001b[38;5;66;03m# temporalily set estimated_norm_scaling_factor to 1.0 so the dataloader works\u001b[39;00m\n\u001b[32m    450\u001b[39m     \u001b[38;5;28mself\u001b[39m.estimated_norm_scaling_factor = \u001b[32m1.0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     acts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    452\u001b[39m     \u001b[38;5;28mself\u001b[39m.estimated_norm_scaling_factor = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    453\u001b[39m     norms_per_batch.append(acts.norm(dim=-\u001b[32m1\u001b[39m).mean().item())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/activations_store.py:784\u001b[39m, in \u001b[36mActivationsStore.next_batch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    778\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    779\u001b[39m \u001b[33;03mGet the next batch from the current DataLoader.\u001b[39;00m\n\u001b[32m    780\u001b[39m \u001b[33;03mIf the DataLoader is exhausted, refill the buffer and create a new DataLoader.\u001b[39;00m\n\u001b[32m    781\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    783\u001b[39m     \u001b[38;5;66;03m# Try to get the next batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataloader\u001b[49m)\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    786\u001b[39m     \u001b[38;5;66;03m# If the DataLoader is exhausted, create a new one\u001b[39;00m\n\u001b[32m    787\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataloader = \u001b[38;5;28mself\u001b[39m.get_data_loader()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/activations_store.py:489\u001b[39m, in \u001b[36mActivationsStore.dataloader\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Iterator[Any]:\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataloader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m         \u001b[38;5;28mself\u001b[39m._dataloader = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_data_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataloader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/activations_store.py:758\u001b[39m, in \u001b[36mActivationsStore.get_data_loader\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    752\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    753\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mWe were unable to fill up the buffer directly after starting a new epoch. This could indicate that there are less samples in the dataset than are required to fill up the buffer. Consider reducing batch_size or n_batches_in_buffer. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    754\u001b[39m         )\n\u001b[32m    756\u001b[39m \u001b[38;5;66;03m# 1. # create new buffer by mixing stored and new buffer\u001b[39;00m\n\u001b[32m    757\u001b[39m mixing_buffer = torch.cat(\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     [new_samples, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_buffer\u001b[49m],\n\u001b[32m    759\u001b[39m     dim=\u001b[32m0\u001b[39m,\n\u001b[32m    760\u001b[39m )\n\u001b[32m    762\u001b[39m mixing_buffer = mixing_buffer[torch.randperm(mixing_buffer.shape[\u001b[32m0\u001b[39m])]\n\u001b[32m    764\u001b[39m \u001b[38;5;66;03m# 2.  put 50 % in storage\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/activations_store.py:481\u001b[39m, in \u001b[36mActivationsStore.storage_buffer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstorage_buffer\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> torch.Tensor:\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._storage_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    480\u001b[39m         \u001b[38;5;28mself\u001b[39m._storage_buffer = _filter_buffer_acts(\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhalf_buffer_size\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.exclude_special_tokens\n\u001b[32m    482\u001b[39m         )\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._storage_buffer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/activations_store.py:690\u001b[39m, in \u001b[36mActivationsStore.get_buffer\u001b[39m\u001b[34m(self, n_batches_in_buffer, raise_on_epoch_end, shuffle)\u001b[39m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m refill_batch_idx_start \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[32m    684\u001b[39m     refill_iterator, leave=\u001b[38;5;28;01mFalse\u001b[39;00m, desc=\u001b[33m\"\u001b[39m\u001b[33mRefilling buffer\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    685\u001b[39m ):\n\u001b[32m    686\u001b[39m     \u001b[38;5;66;03m# move batch toks to gpu for model\u001b[39;00m\n\u001b[32m    687\u001b[39m     refill_batch_tokens = \u001b[38;5;28mself\u001b[39m.get_batch_tokens(\n\u001b[32m    688\u001b[39m         raise_at_epoch_end=raise_on_epoch_end\n\u001b[32m    689\u001b[39m     ).to(_get_model_device(\u001b[38;5;28mself\u001b[39m.model))\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m     refill_activations = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrefill_batch_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m     \u001b[38;5;66;03m# move acts back to cpu\u001b[39;00m\n\u001b[32m    692\u001b[39m     refill_activations.to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/activations_store.py:536\u001b[39m, in \u001b[36mActivationsStore.get_activations\u001b[39m\u001b[34m(self, batch_tokens)\u001b[39m\n\u001b[32m    533\u001b[39m     autocast_if_enabled = contextlib.nullcontext()\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m autocast_if_enabled:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     layerwise_activations_cache = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop_at_layer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhook_layer\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprepend_bos\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m    544\u001b[39m layerwise_activations = layerwise_activations_cache[\u001b[38;5;28mself\u001b[39m.hook_name][\n\u001b[32m    545\u001b[39m     :, \u001b[38;5;28mslice\u001b[39m(*\u001b[38;5;28mself\u001b[39m.seqpos_slice)\n\u001b[32m    546\u001b[39m ]\n\u001b[32m    548\u001b[39m n_batches, n_context = layerwise_activations.shape[:\u001b[32m2\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:694\u001b[39m, in \u001b[36mHookedTransformer.run_with_cache\u001b[39m\u001b[34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_with_cache\u001b[39m(\n\u001b[32m    678\u001b[39m     \u001b[38;5;28mself\u001b[39m, *model_args, return_cache_object=\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs\n\u001b[32m    679\u001b[39m ) -> Tuple[\n\u001b[32m   (...)\u001b[39m\u001b[32m    686\u001b[39m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]],\n\u001b[32m    687\u001b[39m ]:\n\u001b[32m    688\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[32m    689\u001b[39m \n\u001b[32m    690\u001b[39m \u001b[33;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[32m    691\u001b[39m \u001b[33;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[32m    692\u001b[39m \u001b[33;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m694\u001b[39m     out, cache_dict = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    697\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[32m    698\u001b[39m         cache = ActivationCache(cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim=\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/transformer_lens/hook_points.py:569\u001b[39m, in \u001b[36mHookedRootModule.run_with_cache\u001b[39m\u001b[34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m cache_dict, fwd, bwd = \u001b[38;5;28mself\u001b[39m.get_caching_hooks(\n\u001b[32m    556\u001b[39m     names_filter,\n\u001b[32m    557\u001b[39m     incl_bwd,\n\u001b[32m   (...)\u001b[39m\u001b[32m    560\u001b[39m     pos_slice=pos_slice,\n\u001b[32m    561\u001b[39m )\n\u001b[32m    563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hooks(\n\u001b[32m    564\u001b[39m     fwd_hooks=fwd,\n\u001b[32m    565\u001b[39m     bwd_hooks=bwd,\n\u001b[32m    566\u001b[39m     reset_hooks_end=reset_hooks_end,\n\u001b[32m    567\u001b[39m     clear_contexts=clear_contexts,\n\u001b[32m    568\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m     model_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[32m    571\u001b[39m         model_out.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:612\u001b[39m, in \u001b[36mHookedTransformer.forward\u001b[39m\u001b[34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[39m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    608\u001b[39m         shortformer_pos_embed = shortformer_pos_embed.to(\n\u001b[32m    609\u001b[39m             devices.get_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m.cfg)\n\u001b[32m    610\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     residual = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    622\u001b[39m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/transformer_lens/components/transformer_block.py:160\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[39m\n\u001b[32m    153\u001b[39m     key_input = attn_in\n\u001b[32m    154\u001b[39m     value_input = attn_in\n\u001b[32m    156\u001b[39m attn_out = (\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[32m    158\u001b[39m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.use_normalization_before_and_after:\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[32m    174\u001b[39m     attn_out = \u001b[38;5;28mself\u001b[39m.ln1_post(attn_out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/transformer_lens/components/abstract_attention.py:260\u001b[39m, in \u001b[36mAbstractAttention.forward\u001b[39m\u001b[34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[39m\n\u001b[32m    258\u001b[39m attn_scores = \u001b[38;5;28mself\u001b[39m.hook_attn_scores(attn_scores)\n\u001b[32m    259\u001b[39m pattern = F.softmax(attn_scores, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m pattern = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m pattern = \u001b[38;5;28mself\u001b[39m.hook_pattern(pattern)  \u001b[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001b[39;00m\n\u001b[32m    262\u001b[39m pattern = pattern.to(\u001b[38;5;28mself\u001b[39m.cfg.dtype)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 8.00 GiB. GPU 0 has a total capacity of 39.50 GiB of which 6.28 GiB is free. Including non-PyTorch memory, this process has 33.21 GiB memory in use. Of the allocated memory 27.81 GiB is allocated by PyTorch, and 4.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# — run training —\n",
    "sparse_autoencoder = SAETrainingRunner(cfg).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refusal (py3.11)",
   "language": "python",
   "name": "refusalkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
