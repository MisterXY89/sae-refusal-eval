{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8c9baf7-5daa-48c8-b172-6b148eec5653",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2876610e-3d52-4a7b-bf0a-86fb6d822a91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wandb MA-sae-train: 03ca085d27243e9a7876d511e0c7402d861df34c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b20880-5f06-467b-bada-ac489d72f9f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sae_lens import SAETrainingRunner, LanguageModelSAERunnerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db3cab4b-8f01-4104-9b44-ef58c723fa6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# —— Schedule & batch sizing —————————————————————————————————————\n",
    "total_steps            = 75_000                                    # ∼75 K gradient updates\n",
    "batch_size_tokens      = 4_096                                     # 4 K tokens per step to keep an A100 busy\n",
    "total_training_tokens  = total_steps * batch_size_tokens          # ≈300 M tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ae3470-1e93-40e5-a34b-80f481274fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# —— Learning‐rate & sparsity warmups —————————————————————————————\n",
    "lr_warmup_steps  = int(0.1 * total_steps)                          # 10% warmup avoids dead features\n",
    "lr_decay_steps   = int(0.2 * total_steps)                          # 20% decay to stabilize later\n",
    "l1_warmup_steps  = int(0.05 * total_steps)                         # 5% warmup on the L1 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d409e9e-c252-4ad1-a039-83541831d820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# —— SAE‐Lens configuration ———————————————————————————————————————\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # — Data & model hooks —\n",
    "    model_name                    = \"EleutherAI/pythia-410m-deduped\",\n",
    "    hook_name                     = \"blocks.4.hook_mlp_out\",  # intervene on layer 4 MLP output\n",
    "    hook_layer                    = 4,\n",
    "    d_in                          = 1024,                    # MLP hidden‐size for Pythia-410m\n",
    "    # dataset_path                  = \"EleutherAI/pile\",       # large, diverse language corpus\n",
    "    dataset_path                  = \"karpathy/tiny_shakespeare\", \n",
    "    is_dataset_tokenized          = False,\n",
    "    streaming                     = True,                     # stream directly from HF\n",
    "\n",
    "    # — SAE architecture & sparsity —\n",
    "    architecture                  = \"standard\",               # L1‐penalized SAE\n",
    "    expansion_factor              = 16,                       # 16× overcomplete basis (≈32 K latents)\n",
    "    l1_coefficient                = 5.0,                      # medium‐strength sparsity\n",
    "    l1_warm_up_steps              = l1_warmup_steps,\n",
    "    mse_loss_normalization        = None,                     # raw MSE\n",
    "\n",
    "    # — Decoder & encoder initialization —\n",
    "    b_dec_init_method             = \"zeros\",                  # start decoder at zero\n",
    "    init_encoder_as_decoder_transpose = True,                # symmetry speeds convergence\n",
    "    decoder_heuristic_init        = False,                     # geometric‐median bias helps stability\n",
    "\n",
    "    # — Optimization & scheduling —\n",
    "    lr                            = 5e-5,                     # standard Adam LR\n",
    "    adam_beta1                    = 0.9,\n",
    "    adam_beta2                    = 0.999,\n",
    "    lr_scheduler_name             = \"constant\",\n",
    "    lr_warm_up_steps              = lr_warmup_steps,\n",
    "    lr_decay_steps                = lr_decay_steps,\n",
    "\n",
    "    # — Context & batch sizing —\n",
    "    train_batch_size_tokens       = batch_size_tokens,\n",
    "    context_size                  = 256,                      # 256-token windows\n",
    "\n",
    "    # — Reporting & logging —\n",
    "    training_tokens               = total_training_tokens,\n",
    "    feature_sampling_window       = 1_000,                    # report every 1 K steps\n",
    "    log_to_wandb                  = True,\n",
    "    wandb_project                 = \"MA-sae-train\",\n",
    "    wandb_log_frequency           = 50,\n",
    "\n",
    "    # — Miscellaneous —\n",
    "    device                        = \"cuda:0\",\n",
    "    seed                          = 42,\n",
    "    n_checkpoints                 = 0,\n",
    "    dtype                         = \"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2585ce3-eb41-44bf-a5d3-302a4a8a6610",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-410m-deduped into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tilman.kerl/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/activations_store.py:301: UserWarning: Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Training SAE:   0%|          | 0/307200000 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  10%|█         | 1/10 [00:00<00:07,  1.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  20%|██        | 2/10 [00:00<00:03,  2.34it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  30%|███       | 3/10 [00:01<00:01,  3.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  40%|████      | 4/10 [00:01<00:01,  4.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  60%|██████    | 6/10 [00:01<00:00,  6.68it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  80%|████████  | 8/10 [00:01<00:00,  7.89it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer: 100%|██████████| 10/10 [00:01<00:00,  8.66it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                 \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  20%|██        | 2/10 [00:00<00:00, 10.19it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  40%|████      | 4/10 [00:00<00:00, 10.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  60%|██████    | 6/10 [00:00<00:00, 10.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  80%|████████  | 8/10 [00:00<00:00, 10.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer: 100%|██████████| 10/10 [00:00<00:00, 10.32it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                 \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  10%|█         | 1/10 [00:00<00:04,  1.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  30%|███       | 3/10 [00:00<00:01,  4.74it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  50%|█████     | 5/10 [00:00<00:00,  6.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  70%|███████   | 7/10 [00:01<00:00,  7.78it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  90%|█████████ | 9/10 [00:01<00:00,  8.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer: 100%|██████████| 10/10 [00:01<00:00,  8.83it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                 \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  10%|█         | 1/10 [00:00<00:04,  1.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  20%|██        | 2/10 [00:00<00:02,  3.54it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  30%|███       | 3/10 [00:00<00:01,  5.01it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  50%|█████     | 5/10 [00:00<00:00,  7.09it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  70%|███████   | 7/10 [00:01<00:00,  8.26it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                \u001b[A\u001b[A\u001b[A/home/tilman.kerl/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/activations_store.py:740: UserWarning: All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "\n",
      "Refilling buffer:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  10%|█         | 1/10 [00:00<00:07,  1.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  30%|███       | 3/10 [00:01<00:02,  3.39it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  50%|█████     | 5/10 [00:01<00:00,  5.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  70%|███████   | 7/10 [00:01<00:00,  6.55it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  90%|█████████ | 9/10 [00:01<00:00,  7.61it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  20%|██        | 2/10 [00:00<00:00, 10.50it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  40%|████      | 4/10 [00:00<00:00, 10.40it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  60%|██████    | 6/10 [00:00<00:00, 10.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer:  80%|████████  | 8/10 [00:00<00:00, 10.23it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Refilling buffer: 100%|██████████| 10/10 [00:00<00:00, 10.29it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Training SAE:   0%|          | 0/307200000 [01:45<?, ?it/s]      \u001b[A\u001b[A\u001b[A\n",
      "                                                                 \n",
      "\n",
      "100| l1_loss: 301.53195 | mse_loss: 8248.15918:   0%|          | 0/307200000 [00:17<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "Refilling buffer:  50%|█████     | 5/10 [00:00<00:00,  6.52it/s] 409600/307200000 [00:17<3:36:11, 23651.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                 409600/307200000 [00:31<3:36:11, 23651.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "200| l1_loss: 592.45117 | mse_loss: 7674.94434:   0%|          | 409600/307200000 [00:32<3:36:11, 23651.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Refilling buffer:  80%|████████  | 8/10 [00:01<00:00,  8.62it/s] 819200/307200000 [00:32<3:20:27, 25472.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                 819200/307200000 [00:44<3:20:27, 25472.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "300| l1_loss: 855.64856 | mse_loss: 6995.69824:   0%|          | 819200/307200000 [00:50<3:20:27, 25472.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "Refilling buffer:  70%|███████   | 7/10 [00:01<00:00,  6.47it/s] 1228800/307200000 [00:50<3:29:04, 24390.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                 1228800/307200000 [01:01<3:29:04, 24390.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "400| l1_loss: 1074.32947 | mse_loss: 6073.96094:   0%|          | 1228800/307200000 [01:05<3:29:04, 24390.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "Refilling buffer:  70%|███████   | 7/10 [00:01<00:00,  6.50it/s]  1638400/307200000 [01:05<3:20:44, 25369.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "                                                                  1638400/307200000 [01:21<3:20:44, 25369.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "500| l1_loss: 1252.22485 | mse_loss: 5360.37598:   1%|          | 1638400/307200000 [01:23<3:20:44, 25369.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "interrupted, saving progress                                      2048000/307200000 [01:23<3:28:10, 24429.81it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "ename": "InterruptedException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mStopIteration\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/activations_store.py:784\u001b[39m, in \u001b[36mActivationsStore.next_batch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    782\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    783\u001b[39m     \u001b[38;5;66;03m# Try to get the next batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataloader)\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    786\u001b[39m     \u001b[38;5;66;03m# If the DataLoader is exhausted, create a new one\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/utils/data/dataloader.py:763\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m     index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    764\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._dataset_fetcher.fetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/utils/data/dataloader.py:698\u001b[39m, in \u001b[36m_BaseDataLoaderIter._next_index\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m._sampler_iter)\n",
      "\u001b[31mStopIteration\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mInterruptedException\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# — Run!\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sparse_autoencoder = \u001b[43mSAETrainingRunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/sae_training_runner.py:112\u001b[39m, in \u001b[36mSAETrainingRunner.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    103\u001b[39m trainer = SAETrainer(\n\u001b[32m    104\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    105\u001b[39m     sae=\u001b[38;5;28mself\u001b[39m.sae,\n\u001b[32m   (...)\u001b[39m\u001b[32m    108\u001b[39m     cfg=\u001b[38;5;28mself\u001b[39m.cfg,\n\u001b[32m    109\u001b[39m )\n\u001b[32m    111\u001b[39m \u001b[38;5;28mself\u001b[39m._compile_if_needed()\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m sae = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_trainer_with_interruption_handling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cfg.log_to_wandb:\n\u001b[32m    115\u001b[39m     wandb.finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/sae_training_runner.py:151\u001b[39m, in \u001b[36mSAETrainingRunner.run_trainer_with_interruption_handling\u001b[39m\u001b[34m(self, trainer)\u001b[39m\n\u001b[32m    148\u001b[39m     signal.signal(signal.SIGTERM, interrupt_callback)\n\u001b[32m    150\u001b[39m     \u001b[38;5;66;03m# train SAE\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     sae = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, InterruptedException):\n\u001b[32m    154\u001b[39m     logger.warning(\u001b[33m\"\u001b[39m\u001b[33minterrupted, saving progress\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/sae_trainer.py:182\u001b[39m, in \u001b[36mSAETrainer.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# Train loop\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_training_tokens < \u001b[38;5;28mself\u001b[39m.cfg.total_training_tokens:\n\u001b[32m    181\u001b[39m     \u001b[38;5;66;03m# Do a training step.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     layer_acts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactivations_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[32m0\u001b[39m, :].to(\n\u001b[32m    183\u001b[39m         \u001b[38;5;28mself\u001b[39m.sae.device\n\u001b[32m    184\u001b[39m     )\n\u001b[32m    185\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_training_tokens += \u001b[38;5;28mself\u001b[39m.cfg.train_batch_size_tokens\n\u001b[32m    187\u001b[39m     step_output = \u001b[38;5;28mself\u001b[39m._train_step(sae=\u001b[38;5;28mself\u001b[39m.sae, sae_in=layer_acts)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/activations_store.py:787\u001b[39m, in \u001b[36mActivationsStore.next_batch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataloader)\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    786\u001b[39m     \u001b[38;5;66;03m# If the DataLoader is exhausted, create a new one\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataloader = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_data_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataloader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/activations_store.py:736\u001b[39m, in \u001b[36mActivationsStore.get_data_loader\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    732\u001b[39m batch_size = \u001b[38;5;28mself\u001b[39m.train_batch_size_tokens\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    735\u001b[39m     new_samples = _filter_buffer_acts(\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhalf_buffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_on_epoch_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[32m    737\u001b[39m         \u001b[38;5;28mself\u001b[39m.exclude_special_tokens,\n\u001b[32m    738\u001b[39m     )\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    740\u001b[39m     warnings.warn(\n\u001b[32m    741\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAll samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    742\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/activations_store.py:687\u001b[39m, in \u001b[36mActivationsStore.get_buffer\u001b[39m\u001b[34m(self, n_batches_in_buffer, raise_on_epoch_end, shuffle)\u001b[39m\n\u001b[32m    677\u001b[39m new_buffer_token_ids = torch.zeros(\n\u001b[32m    678\u001b[39m     (total_size, training_context_size),\n\u001b[32m    679\u001b[39m     dtype=torch.long,\n\u001b[32m    680\u001b[39m     device=\u001b[38;5;28mself\u001b[39m.device,\n\u001b[32m    681\u001b[39m )\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m refill_batch_idx_start \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[32m    684\u001b[39m     refill_iterator, leave=\u001b[38;5;28;01mFalse\u001b[39;00m, desc=\u001b[33m\"\u001b[39m\u001b[33mRefilling buffer\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    685\u001b[39m ):\n\u001b[32m    686\u001b[39m     \u001b[38;5;66;03m# move batch toks to gpu for model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m     refill_batch_tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m        \u001b[49m\u001b[43mraise_at_epoch_end\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraise_on_epoch_end\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.to(_get_model_device(\u001b[38;5;28mself\u001b[39m.model))\n\u001b[32m    690\u001b[39m     refill_activations = \u001b[38;5;28mself\u001b[39m.get_activations(refill_batch_tokens)\n\u001b[32m    691\u001b[39m     \u001b[38;5;66;03m# move acts back to cpu\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/activations_store.py:506\u001b[39m, in \u001b[36mActivationsStore.get_batch_tokens\u001b[39m\u001b[34m(self, batch_size, raise_at_epoch_end)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[32m    505\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m         sequences.append(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterable_sequences))\n\u001b[32m    507\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    508\u001b[39m         \u001b[38;5;28mself\u001b[39m.iterable_sequences = \u001b[38;5;28mself\u001b[39m._iterate_tokenized_sequences()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/training/activations_store.py:361\u001b[39m, in \u001b[36mActivationsStore._iterate_tokenized_sequences\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    359\u001b[39m tokenizer = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mtokenizer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    360\u001b[39m bos_token_id = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tokenizer.bos_token_id\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m concat_and_batch_sequences(\n\u001b[32m    362\u001b[39m     tokens_iterator=\u001b[38;5;28mself\u001b[39m._iterate_raw_dataset_tokens(),\n\u001b[32m    363\u001b[39m     context_size=\u001b[38;5;28mself\u001b[39m.context_size,\n\u001b[32m    364\u001b[39m     begin_batch_token_id=(bos_token_id \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prepend_bos \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    365\u001b[39m     begin_sequence_token_id=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    366\u001b[39m     sequence_separator_token_id=(\n\u001b[32m    367\u001b[39m         bos_token_id \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prepend_bos \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    368\u001b[39m     ),\n\u001b[32m    369\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/utils/_contextlib.py:57\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     55\u001b[39m             \u001b[38;5;66;03m# Pass the last request to the generator and get its response\u001b[39;00m\n\u001b[32m     56\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m                 response = gen.send(request)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# We let the exceptions raised above by the generator's `.throw` or\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# `.send` methods bubble up to our caller, except for StopIteration\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# The generator informed us that it is done: take whatever its\u001b[39;00m\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# returned value (if any) was and indicate that we're done too\u001b[39;00m\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# by returning it (see docs for python's return-statement).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/tokenization_and_batching.py:88\u001b[39m, in \u001b[36mconcat_and_batch_sequences\u001b[39m\u001b[34m(tokens_iterator, context_size, begin_batch_token_id, begin_sequence_token_id, sequence_separator_token_id)\u001b[39m\n\u001b[32m     86\u001b[39m is_start_of_sequence = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m total_toks - offset > \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     batch, offset = \u001b[43m_add_tokens_to_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_start_of_sequence\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_start_of_sequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbegin_batch_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbegin_batch_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbegin_sequence_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbegin_sequence_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43msequence_separator_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msequence_separator_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m     is_start_of_sequence = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m batch.shape[\u001b[32m0\u001b[39m] == context_size:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/tokenization_and_batching.py:30\u001b[39m, in \u001b[36m_add_tokens_to_batch\u001b[39m\u001b[34m(batch, tokens, offset, context_size, is_start_of_sequence, begin_batch_token_id, begin_sequence_token_id, sequence_separator_token_id)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# add the BOS token to the start if needed\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m begin_batch_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m         begin_batch_token_id_tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mbegin_batch_token_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m first_token != begin_batch_token_id_tensor:\n\u001b[32m     34\u001b[39m             prefix_toks.insert(\u001b[32m0\u001b[39m, begin_batch_token_id_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/sae_lens/sae_training_runner.py:27\u001b[39m, in \u001b[36minterrupt_callback\u001b[39m\u001b[34m(sig_num, stack_frame)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minterrupt_callback\u001b[39m(sig_num: Any, stack_frame: Any):  \u001b[38;5;66;03m# noqa: ARG001\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InterruptedException()\n",
      "\u001b[31mInterruptedException\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# — Run!\n",
    "sparse_autoencoder = SAETrainingRunner(cfg).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97650fa2-04e4-406a-9deb-19eb077330d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Prepare SAE dict for upload\n",
    "#    use the hook_name (e.g. \"blocks.4.hook_mlp_out\") as the folder key\n",
    "sae_dict = {\n",
    "    cfg.hook_name: sparse_autoencoder\n",
    "}\n",
    "\n",
    "# 3) Upload to HuggingFace\n",
    "upload_saes_to_huggingface(\n",
    "    saes=sae_dict,\n",
    "    hf_repo_id=\"your-username/sae-pythia-410m-deduped\",  # your target repo\n",
    "    private=False                                       # set True if you want a private repo\n",
    ")\n",
    "\n",
    "print(f\"✅ Uploaded SAE to https://huggingface.co/{'your-username/sae-pythia-410m-deduped'}\")\n",
    "\n",
    "Notes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refusal (py3.11)",
   "language": "python",
   "name": "refusalkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
