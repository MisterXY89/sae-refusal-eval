{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36db2dad-552c-41fe-b3f7-22c1358fb178",
   "metadata": {},
   "source": [
    "# Model Probing - Identifying most potent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e10e6-25cd-498d-aab0-3b4d7392dfc7",
   "metadata": {},
   "source": [
    "Here we use the linear probing as proposed by Ardit et al. and implemented by Kissane et al.  to identify the most promising layer for our further SAE training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb13073-ebd4-4ad4-b877-2fd13b909485",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5a29736-3b11-4758-ab64-f39b76f11170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import functools\n",
    "from colorama import Fore, Style\n",
    "import textwrap\n",
    "from jaxtyping import Float\n",
    "import einops\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformer_lens\n",
    "from sae_lens import SAE\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from huggingface_hub import whoami, login, notebook_login\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "\n",
    "from jaxtyping import Int\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "925c3064-c84c-4fa2-af53-ac7056264223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data_tools.instructions import get_harmful_instructions, get_harmless_instructions\n",
    "from utils.templates import PYTHIA_TEMPLATE\n",
    "from utils.generation import ( \n",
    "    format_instruction, tokenize_instructions\n",
    ")\n",
    "import steering.linear_probing as lp_steer\n",
    "import refusal.linear_probing as lp_refuse\n",
    "from evaluation.refusal import (\n",
    "    get_refusal_scores, get_wildguard_refusal_score\n",
    ")\n",
    "from config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5e667ac-d248-4fcd-9afb-e0e35c67f1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "login(config.credentials.hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54011e6c-070c-4bb0-b31e-63e58780758d",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d00c759e-f3b0-4b0b-b105-59aacd86644e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_MODEL_NAME = \"EleutherAI/pythia-410m-deduped\"\n",
    "INSTRUCT_MODEL_NAME = \"SummerSigh/Pythia410m-V0-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb95c367-68dc-4930-926c-a601b11537f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "harmless_inst_train, harmless_inst_test = get_harmless_instructions()\n",
    "harmful_inst_train, harmful_inst_test = get_harmful_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0094aea-9741-49b4-9a8d-5143b055cbd6",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef3c9e4f-e231-409b-93e7-edbb59f5366a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-410m-deduped into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = HookedTransformer.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    default_padding_side='left',\n",
    "\n",
    ")\n",
    "base_model.tokenizer.padding_side = 'left'\n",
    "base_model.tokenizer.add_special_tokens({'pad_token': '<|padding|>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eec5828e-cced-4746-af07-45356fbd9b98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base_model_layer = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd5cef39-50fb-4a85-b193-fb383fb467ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model_tokenize_instructions_fn = lambda instructions: tokenize_instructions(\n",
    "    tokenizer=base_model.tokenizer,\n",
    "    instructions=instructions,\n",
    "    template=PYTHIA_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e23d702-7c72-44c5-b615-8b703436ca89",
   "metadata": {},
   "source": [
    "### Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "977bfa15-c76a-42c9-a565-3ceb0b6dbdb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-410m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "instruct_model_hf = AutoModelForCausalLM.from_pretrained(INSTRUCT_MODEL_NAME)\n",
    "\n",
    "instruct_model = HookedTransformer.from_pretrained(\n",
    "    \"EleutherAI/pythia-410m-deduped\",\n",
    "    hf_model=instruct_model_hf,\n",
    "    default_padding_side='left',\n",
    "  )\n",
    "\n",
    "instruct_tokenizer = AutoTokenizer.from_pretrained(INSTRUCT_MODEL_NAME)\n",
    "instruct_tokenizer.padding_side = 'left'\n",
    "instruct_tokenizer.pad_token = instruct_tokenizer.eos_token\n",
    "\n",
    "# chat_model.tokenizer.add_special_tokens({'pad_token': '<|padding|>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbaa22d-1d9c-42b6-9c3d-8c6dbcfd8cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruct_model_layer = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c582d075-e2e4-4907-ab4e-9eae0ae1c6d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruct_model_tokenize_instructions_fn = lambda instructions: tokenize_instructions(\n",
    "    tokenizer=instruct_tokenizer,\n",
    "    instructions=instructions,\n",
    "    template=PYTHIA_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2dd52e-84a4-4b2e-b775-aaddc1c7cf1a",
   "metadata": {},
   "source": [
    "## Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fcd01cd-c932-4ee7-8cbe-e04cae809cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "candidate_layers = [2,4,8,12,14,16,18,20,22,23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11cc7cea-83ab-4e6f-ac8d-8993cec8d93c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []  # will hold tuples (layer, Δℓ, hooked_refusal, wildguard_refusal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1007569-0787-423a-9f6c-9ee7f76a050e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# as our experiments in base_refusal.ipynb showed:\n",
    "# (we use the wildguard score)\n",
    "baseline_refusal_rate = 0.11538461538461539"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5393b1c8-cf65-4858-be83-27899a251e44",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 50 pairs to compute refusal direction\n",
      "saved refusal direction to refusal_dir_EleutherAI/pythia-410m-deduped_layer_2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:10<00:00,  5.46s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 0/104 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: N/A\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: N/A\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: N/A\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: N/A\n",
      "Response refusal: no\n",
      "Response refusal: no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 86/104 [00:00<00:00, 855.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: N/A\n",
      "Response refusal: no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:00<00:00, 872.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>  2 -0.04807692307692309\n",
      "using 50 pairs to compute refusal direction\n",
      "saved refusal direction to refusal_dir_EleutherAI/pythia-410m-deduped_layer_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:12<00:00,  5.55s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 0/104 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: yes\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 84/104 [00:00<00:00, 838.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:00<00:00, 856.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>  4 0.11538461538461539\n",
      "using 50 pairs to compute refusal direction\n",
      "saved refusal direction to refusal_dir_EleutherAI/pythia-410m-deduped_layer_8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:11<00:00,  5.50s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 0/104 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: N/A\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: N/A\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: yes\n",
      "Response refusal: no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 95/104 [00:00<00:00, 949.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: N/A\n",
      "Response refusal: no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:00<00:00, 961.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>  8 0.009615384615384609\n",
      "using 50 pairs to compute refusal direction\n",
      "saved refusal direction to refusal_dir_EleutherAI/pythia-410m-deduped_layer_12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:11<00:00,  5.50s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 0/104 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: N/A\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: N/A\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: N/A\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 87/104 [00:00<00:00, 863.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: no\n",
      "Response refusal: yes\n",
      "Response refusal: N/A\n",
      "Response refusal: no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:00<00:00, 878.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>  12 0.11538461538461539\n",
      "using 50 pairs to compute refusal direction\n",
      "saved refusal direction to refusal_dir_EleutherAI/pythia-410m-deduped_layer_14.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:11<00:00,  5.51s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.46 GiB. GPU 0 has a total capacity of 44.45 GiB of which 1.45 GiB is free. Including non-PyTorch memory, this process has 42.99 GiB memory in use. Of the allocated memory 40.67 GiB is allocated by PyTorch, and 2.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m hooked_refusal_rate = get_refusal_scores(hooked_gens)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# e) (Optionally) measure a secondary metric: how WildGuard scores these outputs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m wildguard_refusal_rate = \u001b[43mget_wildguard_refusal_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mharmful_inst_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhooked_gens\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# f) Δℓ = hooked_refusal – baseline_refusal\u001b[39;00m\n\u001b[32m     48\u001b[39m delta = wildguard_refusal_rate - baseline_refusal_rate\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mech-interp/src/evaluation/refusal.py:88\u001b[39m, in \u001b[36mget_wildguard_refusal_score\u001b[39m\u001b[34m(instructions, generations, v)\u001b[39m\n\u001b[32m     86\u001b[39m max_new_tokens = \u001b[32m32\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     outputs = \u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m refusals = \u001b[32m0\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# use attention mask to determine actual input lengths (to slice generated tokens)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/transformers/generation/utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/transformers/generation/utils.py:3431\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3428\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3430\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m3431\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3432\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:810\u001b[39m, in \u001b[36mMistralForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m output_hidden_states = (\n\u001b[32m    806\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    807\u001b[39m )\n\u001b[32m    809\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    823\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    824\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/transformers/utils/generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:536\u001b[39m, in \u001b[36mMistralModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    524\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    525\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    526\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    533\u001b[39m         position_embeddings,\n\u001b[32m    534\u001b[39m     )\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    537\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    548\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:265\u001b[39m, in \u001b[36mMistralDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m residual = hidden_states\n\u001b[32m    264\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    268\u001b[39m outputs = (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/refusal/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:59\u001b[39m, in \u001b[36mMistralMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.46 GiB. GPU 0 has a total capacity of 44.45 GiB of which 1.45 GiB is free. Including non-PyTorch memory, this process has 42.99 GiB memory in use. Of the allocated memory 40.67 GiB is allocated by PyTorch, and 2.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# 3. For each layer ℓ, extract r̂ℓ, apply an addition‐hook, generate, and measure Δℓ\n",
    "for l in candidate_layers:\n",
    "    # a) Compute (or load) the normalized refusal direction at layer ℓ\n",
    "    r_hat = lp_refuse.extract_refusal_direction(\n",
    "        model=base_model,\n",
    "        model_name=BASE_MODEL_NAME,\n",
    "        harmless_inst_train=harmless_inst_train,\n",
    "        harmful_inst_train=harmful_inst_train,\n",
    "        n_inst_train=50,\n",
    "        layer=l,\n",
    "        pos=-1,  # final token\n",
    "        pythia_template=PYTHIA_TEMPLATE,\n",
    "        tokenize_instructions_fn=base_model_tokenize_instructions_fn,\n",
    "        force=True,  # recompute even if the file exists\n",
    "    )\n",
    "\n",
    "    # b) Build the “addition” hook at layer ℓ with steering_coef=3.2 (arbitrary scale)\n",
    "    hook_dict = lp_steer.get_refusal_direction_hooks(\n",
    "        model=base_model,\n",
    "        refusal_dir=r_hat,\n",
    "        act_add_hook=lp_steer.act_add_hook,\n",
    "        direction_ablation_hook=lp_steer.direction_ablation_hook,\n",
    "        intervention_type=\"addition\",\n",
    "        steering_coef=2.0,\n",
    "        layer=l,\n",
    "    )\n",
    "\n",
    "    # c) Generate on harmful_inst_test *with* the hook active\n",
    "    hooked_gens = lp_steer.get_generations(\n",
    "        base_model,\n",
    "        instructions=harmful_inst_test,\n",
    "        tokenize_instructions_fn=base_model_tokenize_instructions_fn,\n",
    "        fwd_hooks=hook_dict,\n",
    "        max_tokens_generated=100,\n",
    "        batch_size=8,\n",
    "    )\n",
    "\n",
    "    # d) Compute “refusal‐rate” under intervention\n",
    "    hooked_refusal_rate = get_refusal_scores(hooked_gens)\n",
    "\n",
    "    # e) (Optionally) measure a secondary metric: how WildGuard scores these outputs\n",
    "    wildguard_refusal_rate = get_wildguard_refusal_score(\n",
    "        instructions=harmful_inst_test,\n",
    "        generations=hooked_gens\n",
    "    )\n",
    "\n",
    "    # f) Δℓ = hooked_refusal – baseline_refusal\n",
    "    delta = wildguard_refusal_rate - baseline_refusal_rate\n",
    "\n",
    "    results.append((l, delta, hooked_refusal_rate, wildguard_refusal_rate))\n",
    "    print(\">>>>> \", l, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9588652-7b89-45ea-b3c4-3a56d9f33573",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer scan results (layer, Δℓ, hooked_refusal, wildguard_refusal):\n",
      "  ℓ= 4 → Δ=0.115, hooked=0.000, wg=0.231\n",
      "  ℓ=12 → Δ=0.115, hooked=0.000, wg=0.231\n",
      "  ℓ= 8 → Δ=0.010, hooked=0.000, wg=0.125\n",
      "  ℓ= 2 → Δ=-0.048, hooked=0.000, wg=0.067\n",
      "  ℓ= 4 → Δ=-0.115, hooked=0.000, wg=0.231\n",
      "  ℓ= 8 → Δ=-0.115, hooked=0.000, wg=0.125\n",
      "  ℓ=12 → Δ=-0.115, hooked=0.000, wg=0.231\n",
      "  ℓ=16 → Δ=-0.115, hooked=0.000, wg=0.125\n",
      "  ℓ=20 → Δ=-0.115, hooked=0.000, wg=0.125\n",
      "\n",
      "=> Selected layer ℓ* = 4 (Δℓ* = 0.115)\n"
     ]
    }
   ],
   "source": [
    "# Sort and select the \"most promising\"layer by maximal Δℓ\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "best_layer, best_delta, best_hooked, best_wg = results[0]\n",
    "\n",
    "print(f\"Layer scan results (layer, Δℓ, hooked_refusal, wildguard_refusal):\")\n",
    "for layer, delta, hr, wr in results:\n",
    "    print(f\"  ℓ={layer:2d} → Δ={delta:.3f}, hooked={hr:.3f}, wg={wr:.3f}\")\n",
    "\n",
    "print(f\"\\n=> Selected layer ℓ* = {best_layer} (Δℓ* = {best_delta:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refusal (py3.11)",
   "language": "python",
   "name": "refusalkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
