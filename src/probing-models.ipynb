{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36db2dad-552c-41fe-b3f7-22c1358fb178",
   "metadata": {},
   "source": [
    "# Model Probing - Identifying most potent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e10e6-25cd-498d-aab0-3b4d7392dfc7",
   "metadata": {},
   "source": [
    "Here we use the linear probing as proposed by Ardit et al. and implemented by Kissane et al.  to identify the most promising layer for our further SAE training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb13073-ebd4-4ad4-b877-2fd13b909485",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a29736-3b11-4758-ab64-f39b76f11170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import functools\n",
    "from colorama import Fore, Style\n",
    "import textwrap\n",
    "from jaxtyping import Float\n",
    "import einops\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformer_lens\n",
    "from sae_lens import SAE\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from huggingface_hub import whoami, login, notebook_login\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "\n",
    "from jaxtyping import Int\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "925c3064-c84c-4fa2-af53-ac7056264223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data_tools.instructions import get_harmful_instructions, get_harmless_instructions\n",
    "from utils.templates import PYTHIA_TEMPLATE\n",
    "from utils.generation import ( \n",
    "    format_instruction, tokenize_instructions\n",
    ")\n",
    "import steering.linear_probing as lp_steer\n",
    "import refusal.linear_probing as lp_refuse\n",
    "from evaluation.refusal import (\n",
    "    get_refusal_scores, get_wildguard_refusal_score\n",
    ")\n",
    "from config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e667ac-d248-4fcd-9afb-e0e35c67f1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "login(config.credentials.hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54011e6c-070c-4bb0-b31e-63e58780758d",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb95c367-68dc-4930-926c-a601b11537f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25058 6265\n",
      "416 104\n"
     ]
    }
   ],
   "source": [
    "harmless_inst_train, harmless_inst_test = get_harmless_instructions()\n",
    "harmful_inst_train, harmful_inst_test = get_harmful_instructions()\n",
    "\n",
    "print(len(harmless_inst_train), len(harmless_inst_test))\n",
    "print(len(harmful_inst_train), len(harmful_inst_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "173c282d-33c9-4968-a254-f15e528f1134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(rr, orr):\n",
    "    # refusal-rate = recall\n",
    "    # over-rr = precision\n",
    "    return 2*((orr*rr)/(orr+rr))\n",
    "    \n",
    "\n",
    "def compute_nbgs(rr, orr, rr0, orr0):\n",
    "    if rr0 >= 1.0:\n",
    "        raise ValueError(\"rr0 must be < 1 to avoid division by zero\")\n",
    "    if orr0 <= 0.0:\n",
    "        raise ValueError(\"orr0 must be > 0 to avoid division by zero\")\n",
    "    gain = (rr - rr0) / (1 - rr0)\n",
    "    penalty = (orr - orr0) / orr0\n",
    "    return gain - penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44379aa-34b2-4a59-9b8e-2358cf09ec41",
   "metadata": {},
   "source": [
    "## Probing SmolLM-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e65feb10-03d4-40eb-874c-89bb7d618006",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from steering.hf_linear_probing import ActivationSteering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07f3f195-1e30-4582-9234-b5327eedf1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the steering handler...\n",
      "Using device: cuda\n",
      "Model 'HuggingFaceTB/SmolLM2-135M' has 30 layers.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Initialization ---\n",
    "MODEL_NAME = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "STEERING_COEF = 18.0\n",
    "NUM_TEST_SAMPLES = 100\n",
    "NUM_TRAIN_SAMPLES = 500\n",
    "\n",
    "print(\"Initializing the steering handler...\")\n",
    "hf_steering_handler = ActivationSteering(MODEL_NAME)\n",
    "num_layers = hf_steering_handler.model.config.num_hidden_layers\n",
    "print(f\"Model '{MODEL_NAME}' has {num_layers} layers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d7420ee-1763-4bfc-8188-91848de72f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating baseline performance (without steering) for HuggingFaceTB/SmolLM2-135M...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503dfdec7e6b4573ae5fc228f466abe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|██████████| 100/100 [00:00<00:00, 20365.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR2 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|██████████| 100/100 [00:00<00:00, 20064.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORR2 0.0\n",
      "Baseline Refusal Rate (RR0): 0.0000\n",
      "Baseline Over-Refusal Rate (ORR0): 0.0700\n"
     ]
    }
   ],
   "source": [
    "# --- calculate Baseline (Unsteered) Performance ---\n",
    "print(f\"\\nCalculating baseline performance (without steering) for {MODEL_NAME}...\")\n",
    "\n",
    "# Generate baseline responses for harmful instructions\n",
    "baseline_harmful_responses = [\n",
    "    hf_steering_handler.generate(prompt, steering_coef=0.0) \n",
    "    for prompt in harmful_inst_test[:NUM_TEST_SAMPLES]\n",
    "]\n",
    "baseline_refusal_rate = get_wildguard_refusal_score(\n",
    "    instructions=harmful_inst_test[:NUM_TEST_SAMPLES],\n",
    "    generations=baseline_harmful_responses\n",
    ")\n",
    "rr_2 = get_refusal_scores(baseline_harmful_responses)\n",
    "print(\"RR2\", rr_2)\n",
    "\n",
    "# Generate baseline responses for harmless instructions\n",
    "baseline_harmless_responses = [\n",
    "    hf_steering_handler.generate(prompt, steering_coef=0.0) \n",
    "    for prompt in harmless_inst_test[:NUM_TEST_SAMPLES]\n",
    "]\n",
    "baseline_over_refusal_rate = get_wildguard_refusal_score(\n",
    "    instructions=harmless_inst_test[:NUM_TEST_SAMPLES],\n",
    "    generations=baseline_harmless_responses\n",
    ")\n",
    "orr_2 = get_refusal_scores(baseline_harmless_responses)\n",
    "print(\"ORR2\", orr_2)\n",
    "\n",
    "print(f\"Baseline Refusal Rate (RR0): {baseline_refusal_rate:.4f}\")\n",
    "print(f\"Baseline Over-Refusal Rate (ORR0): {baseline_over_refusal_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5d6128-1991-4685-b93f-9e425f2c02f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting layer sweep with steering_coef = 18.0 ---\n",
      "\n",
      "----- Processing Layer 0 -----\n",
      "Creating steering vector at layer 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting activations: 100%|██████████| 500/500 [00:04<00:00, 110.58it/s]\n",
      "Collecting activations: 100%|██████████| 416/416 [00:03<00:00, 110.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steering vector created and normalized.\n",
      "   Generating steered responses for harmful test set...\n",
      "   Generating steered responses for harmless test set...\n"
     ]
    }
   ],
   "source": [
    "# --- Iterate Over All Layers ---\n",
    "results = []\n",
    "print(f\"\\n--- Starting layer sweep with steering_coef = {STEERING_COEF} ---\")\n",
    "\n",
    "for l in range(num_layers):\n",
    "    print(f\"\\n----- Processing Layer {l} -----\")\n",
    "    \n",
    "    # 1) Collect activations and create steering vector for the current layer\n",
    "    hf_steering_handler.create_steering_vector(\n",
    "        harmful_prompts=harmful_inst_train[:NUM_TRAIN_SAMPLES], \n",
    "        harmless_prompts=harmless_inst_train[:NUM_TRAIN_SAMPLES],\n",
    "        layer_idx=l\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(\"   Generating steered responses for harmful test set...\")\n",
    "    harmful_responses = [\n",
    "        hf_steering_handler.generate(prompt, steering_coef=STEERING_COEF) \n",
    "        for prompt in harmful_inst_test[:NUM_TEST_SAMPLES]\n",
    "    ]\n",
    "    \n",
    "    print(\"   Generating steered responses for harmless test set...\")\n",
    "    harmless_responses = [\n",
    "        hf_steering_handler.generate(prompt, steering_coef=STEERING_COEF) \n",
    "        for prompt in harmless_inst_test[:NUM_TEST_SAMPLES]\n",
    "    ]\n",
    "\n",
    "    # 3) Evaluate responses\n",
    "    print(\"   Evaluating steered responses...\")\n",
    "    rr = get_wildguard_refusal_score(\n",
    "        instructions=harmful_inst_test[:NUM_TEST_SAMPLES],\n",
    "        generations=harmful_responses\n",
    "    )  # Refusal on harmful\n",
    "\n",
    "    rr_2 = get_refusal_scores(harmful_responses)\n",
    "    print(rr_2)\n",
    "\n",
    "    orr = get_wildguard_refusal_score(\n",
    "        instructions=harmless_inst_test[:NUM_TEST_SAMPLES],\n",
    "        generations=harmless_responses\n",
    "    )  # Over-refusal on harmless\n",
    "\n",
    "    orr_2 = get_refusal_scores(harmless_responses)\n",
    "    print(orr_2)\n",
    "\n",
    "    f1 = compute_f1(\n",
    "        rr=rr,\n",
    "        orr=orr,        \n",
    "    )\n",
    "\n",
    "    results.append((l, f1, rr, orr))\n",
    "    print(f\">>>>> Layer {l} | F1: {nbgs:.4f} | RR: {rr:.4f} | ORR: {orr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3322b005-10a0-40e3-8d80-f3cda8105b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Analysis ---\n",
    "if results:\n",
    "    # Find the layer with the highest NBGS\n",
    "    best_layer, best_nbgs, best_rr, best_orr = max(results, key=lambda item: item[1])\n",
    "    \n",
    "    print(\"\\n\\n========================================\")\n",
    "    print(\"         SWEEP COMPLETE\")\n",
    "    print(\"========================================\")\n",
    "    print(f\"Best result found at Layer {best_layer}:\")\n",
    "    print(f\"  - F1: {best_nbgs:.4f}\")\n",
    "    print(f\"  - Refusal Rate (RR): {best_rr:.4f}\")\n",
    "    print(f\"  - Over-Refusal Rate (ORR): {best_orr:.4f}\")\n",
    "else:\n",
    "    print(\"No results were generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc85474-23f3-41ad-90d1-6017cc35cf04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Pythia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b74466a9-451d-4fc7-a693-16adc25191e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_NAME = \"EleutherAI/pythia-410m-deduped\"\n",
    "INSTRUCT_MODEL_NAME = \"SummerSigh/Pythia410m-V0-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c208a489-5bec-46c9-9477-c8517f64b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as our experiments in base_refusal.ipynb showed:\n",
    "# (we use the wildguard score)\n",
    "baseline_refusal_rate = 0.125\n",
    "baseline_over_refusal_rate = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0094aea-9741-49b4-9a8d-5143b055cbd6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pythia Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c9e4f-e231-409b-93e7-edbb59f5366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = HookedTransformer.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    default_padding_side='left',\n",
    "\n",
    ")\n",
    "base_model.tokenizer.padding_side = 'left'\n",
    "base_model.tokenizer.add_special_tokens({'pad_token': '<|padding|>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec5828e-cced-4746-af07-45356fbd9b98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base_model_layer = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5cef39-50fb-4a85-b193-fb383fb467ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model_tokenize_instructions_fn = lambda instructions: tokenize_instructions(\n",
    "    tokenizer=base_model.tokenizer,\n",
    "    instructions=instructions,\n",
    "    template=PYTHIA_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0aee98-ed1d-4a95-981e-073a3937a751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model.hook_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e23d702-7c72-44c5-b615-8b703436ca89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pytia Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977bfa15-c76a-42c9-a565-3ceb0b6dbdb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruct_model_hf = AutoModelForCausalLM.from_pretrained(INSTRUCT_MODEL_NAME)\n",
    "\n",
    "instruct_model = HookedTransformer.from_pretrained(\n",
    "    \"EleutherAI/pythia-410m-deduped\",\n",
    "    hf_model=instruct_model_hf,\n",
    "    default_padding_side='left',\n",
    "  )\n",
    "\n",
    "instruct_tokenizer = AutoTokenizer.from_pretrained(INSTRUCT_MODEL_NAME)\n",
    "instruct_tokenizer.padding_side = 'left'\n",
    "instruct_tokenizer.pad_token = instruct_tokenizer.eos_token\n",
    "\n",
    "# chat_model.tokenizer.add_special_tokens({'pad_token': '<|padding|>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbaa22d-1d9c-42b6-9c3d-8c6dbcfd8cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruct_model_layer = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c582d075-e2e4-4907-ab4e-9eae0ae1c6d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruct_model_tokenize_instructions_fn = lambda instructions: tokenize_instructions(\n",
    "    tokenizer=instruct_tokenizer,\n",
    "    instructions=instructions,\n",
    "    template=PYTHIA_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2dd52e-84a4-4b2e-b775-aaddc1c7cf1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd01cd-c932-4ee7-8cbe-e04cae809cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "candidate_layers = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc7cea-83ab-4e6f-ac8d-8993cec8d93c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []  # will hold tuples (layer, Δℓ, hooked_refusal, wildguard_refusal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1007569-0787-423a-9f6c-9ee7f76a050e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# as our experiments in base_refusal.ipynb showed:\n",
    "# (we use the wildguard score)\n",
    "# TODO: RECOMPUTE!!!\n",
    "baseline_refusal_rate = 0.125\n",
    "baseline_over_refusal_rate = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5393b1c8-cf65-4858-be83-27899a251e44",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. For each layer ℓ, extract r̂ℓ, apply an addition‐hook, generate, and measure NBGS\n",
    "for l in candidate_layers:\n",
    "    # a) Compute (or load) the normalized refusal direction at layer ℓ\n",
    "    r_hat = lp_refuse.extract_refusal_direction(\n",
    "        model=base_model,\n",
    "        model_name=BASE_MODEL_NAME,\n",
    "        harmless_inst_train=harmless_inst_train,\n",
    "        harmful_inst_train=harmful_inst_train,\n",
    "        n_inst_train=50,\n",
    "        layer=l,\n",
    "        pos=-1,\n",
    "        pythia_template=PYTHIA_TEMPLATE,\n",
    "        tokenize_instructions_fn=base_model_tokenize_instructions_fn,\n",
    "        force=True,\n",
    "    )\n",
    "\n",
    "    # b) Build the “addition” hook at layer ℓ with steering_coef=2.0\n",
    "    hook_dict = lp_steer.get_refusal_direction_hooks(\n",
    "        model=base_model,\n",
    "        refusal_dir=r_hat,\n",
    "        act_add_hook=lp_steer.act_add_hook,\n",
    "        direction_ablation_hook=lp_steer.direction_ablation_hook,\n",
    "        intervention_type=\"multiply\",\n",
    "        steering_coef=2.0,\n",
    "        layer=l,\n",
    "    )\n",
    "\n",
    "    # c) Generate on harmful test prompts with hook active\n",
    "    harmful_gens = lp_steer.get_generations(\n",
    "        base_model,\n",
    "        instructions=harmful_inst_test,\n",
    "        tokenize_instructions_fn=base_model_tokenize_instructions_fn,\n",
    "        fwd_hooks=hook_dict,\n",
    "        max_tokens_generated=32,\n",
    "        batch_size=8,\n",
    "    )\n",
    "    rr = get_wildguard_refusal_score(\n",
    "        instructions=harmful_inst_test,\n",
    "        generations=harmful_gens\n",
    "    )  # refusal on harmful\n",
    "\n",
    "    # d) Generate on harmless test prompts with hook active\n",
    "    harmless_gens = lp_steer.get_generations(\n",
    "        base_model,\n",
    "        instructions=harmless_inst_test,\n",
    "        tokenize_instructions_fn=base_model_tokenize_instructions_fn,\n",
    "        fwd_hooks=hook_dict,\n",
    "        max_tokens_generated=32,\n",
    "        batch_size=8,\n",
    "    )\n",
    "    orr = get_wildguard_refusal_score(\n",
    "        instructions=harmless_inst_test[:],\n",
    "        generations=harmless_gens\n",
    "    )  # over-refusal on harmless\n",
    "\n",
    "    # e) Compute NBGS\n",
    "    nbgs = compute_nbgs(\n",
    "        rr=rr,\n",
    "        orr=orr,\n",
    "        rr0=baseline_refusal_rate,\n",
    "        orr0=baseline_over_refusal_rate,\n",
    "    )\n",
    "\n",
    "    results.append((l, nbgs, rr, orr))\n",
    "    print(f\">>>>> Layer {l} | NBGS: {nbgs:.4f} | RR: {rr:.4f} | ORR: {orr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9588652-7b89-45ea-b3c4-3a56d9f33573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort and select the \"most promising\" layer by maximal NBGS\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "best_layer, best_nbgs, best_rr, best_orr = results[0]\n",
    "\n",
    "print(f\"Layer scan results (layer, NBGS, RR, ORR):\")\n",
    "for layer, nbgs, rr, orr in results:\n",
    "    print(f\"  ℓ={layer:2d} → NBGS={nbgs:.3f}, RR={rr:.3f}, ORR={orr:.3f}\")\n",
    "\n",
    "print(f\"\\n=> Selected layer ℓ* = {best_layer} (NBGS* = {best_nbgs:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refusal (py3.11)",
   "language": "python",
   "name": "refusalkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
