{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Refusal Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to reproduce the findings of \"Base Models Refuse Too\" for the Pythia models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary libraries once, then comment out the installation cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch pandas numpy scikit-learn matplotlib seaborn tqdm ipywidgets sae-lens transformer-lens jaxtyping einops colorama accelerate bitsandbytes>0.37.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import functools\n",
    "from colorama import Fore, Style\n",
    "import textwrap\n",
    "from jaxtyping import Float\n",
    "import einops\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformer_lens\n",
    "from sae_lens import SAE\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "\n",
    "from jaxtyping import Int\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of our own (util) functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_tools.instructions import get_harmful_instructions, get_harmless_instructions\n",
    "from utils.templates import PYTHIA_TEMPLATE\n",
    "from utils.generation import ( \n",
    "    get_generations, format_instruction, tokenize_instructions, \n",
    "    act_add_hook, direction_ablation_hook,\n",
    "    get_refusal_direction_hooks\n",
    ")\n",
    "from utils.refusal import (\n",
    "    get_refusal_scores, extract_refusal_direction,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"pythia-410m\": {\n",
    "        \"base_model\": {},\n",
    "        \"instruct_model\": {},\n",
    "        \"hooked_base_model\": {},\n",
    "        \"hooked_instruct_model\": {}\n",
    "    }\n",
    "}\n",
    "\n",
    "BASE_MODEL_NAME = \"EleutherAI/pythia-410m-deduped\"\n",
    "INSTRUCT_MODEL_NAME = \"SummerSigh/Pythia410m-V0-Instruct\"\n",
    "\n",
    "STEERING_COEFF = 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the data and the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmless_inst_train, harmless_inst_test = get_harmless_instructions()\n",
    "harmful_inst_train, harmful_inst_test = get_harmful_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-410m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "base_model = HookedTransformer.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    default_padding_side='left',\n",
    "\n",
    ")\n",
    "base_model.tokenizer.padding_side = 'left'\n",
    "base_model.tokenizer.add_special_tokens({'pad_token': '<|padding|>'})\n",
    "\n",
    "base_model_layer = 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up our tokenize and generation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_tokenize_instructions_fn = lambda instructions: tokenize_instructions(\n",
    "    tokenizer=base_model.tokenizer,\n",
    "    instructions=instructions,\n",
    "    template=PYTHIA_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Instruct Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we load the model and the set up the respective util functions. As there is now `HookedTransformer` implementation for the Instruct model, we load the HF model directly and pass it along and only specify the architecture in the `from_pretrained` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-410m-deduped into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "instruct_model_hf = AutoModelForCausalLM.from_pretrained(INSTRUCT_MODEL_NAME)\n",
    "\n",
    "instruct_model = HookedTransformer.from_pretrained(\n",
    "    \"EleutherAI/pythia-410m-deduped\",\n",
    "    hf_model=instruct_model_hf,\n",
    "    default_padding_side='left',\n",
    "  )\n",
    "\n",
    "instruct_tokenizer = AutoTokenizer.from_pretrained(INSTRUCT_MODEL_NAME)\n",
    "instruct_tokenizer.padding_side = 'left'\n",
    "instruct_tokenizer.pad_token = instruct_tokenizer.eos_token\n",
    "\n",
    "# chat_model.tokenizer.add_special_tokens({'pad_token': '<|padding|>'})\n",
    "\n",
    "instruct_model_layer = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model_tokenize_instructions_fn = lambda instructions: tokenize_instructions(\n",
    "    tokenizer=instruct_tokenizer,\n",
    "    instructions=instructions,\n",
    "    template=PYTHIA_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refusal Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract the refusal direction from both models, following the SAE Microsoft Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can do the same for the instruct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refusalKernel",
   "language": "python",
   "name": "refusalkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
