{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAE Refusal Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary libraries once, then comment out the installation cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import functools\n",
    "from colorama import Fore, Style\n",
    "import textwrap\n",
    "from jaxtyping import Float, Int\n",
    "import einops\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import transformer_lens\n",
    "# from sae_lens import SAE\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Sparsify Library from EleutherAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to be executed in the shell\n",
    "# !rm x-rf sparsify\n",
    "# !git clone https://github.com/EleutherAI/sparsify.git\n",
    "# !pip install ./sparsify --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sparsify import Sae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of our own (util) functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_tools.instructions import get_harmful_instructions, get_harmless_instructions\n",
    "from utils.templates import PYTHIA_TEMPLATE\n",
    "from utils.generation import ( \n",
    "    format_instruction, tokenize_instructions\n",
    ")\n",
    "import steering.linear_probing as lp_steer\n",
    "import refusal.linear_probing as lp_refuse\n",
    "\n",
    "from refusal.sae.sparsify.latent_features import get_latent_feature_stats as sparsify_get_latent_feature_stats\n",
    "from refusal.sae.sparsify import utils as sparsify_utils\n",
    "\n",
    "from evaluation.refusal import (\n",
    "    get_refusal_scores, get_wildguard_refusal_score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"pythia-410m\": {\n",
    "        \"base_model\": {},\n",
    "        \"instruct_model\": {},\n",
    "        \"hooked_base_model\": {},\n",
    "        \"hooked_instruct_model\": {}\n",
    "    }\n",
    "}\n",
    "\n",
    "BASE_MODEL_NAME = \"EleutherAI/pythia-410m-deduped\"\n",
    "INSTRUCT_MODEL_NAME = \"SummerSigh/Pythia410m-V0-Instruct\"\n",
    "\n",
    "STEERING_COEFF = 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the data and the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmless_inst_train, harmless_inst_test = get_harmless_instructions()\n",
    "harmful_inst_train, harmful_inst_test = get_harmful_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Base Model & Base SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "/home/tilman.kerl/miniconda3/envs/refusal/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token # Use eos_token as pad_token\n",
    "\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,         # Enable 8-bit quantization.\n",
    "#     llm_int8_threshold=6.0,      # (Optional) Set the outlier threshold.\n",
    "#     # You can also set other parameters here if needed.\n",
    "# )\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    load_in_8bit=True,\n",
    "    # quantization_config=quant_config,\n",
    "    # device_map=\"auto\",\n",
    "    device_map={\"\": \"cuda:0\"},    # pin everything to cuda:0\n",
    "    output_hidden_states=True,  # ensure hidden states are returned\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up our tokenize and generation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_tokenize_instructions_fn = lambda instructions: tokenize_instructions(\n",
    "    tokenizer=base_tokenizer,\n",
    "    instructions=instructions,\n",
    "    template=PYTHIA_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Instruct Model & Base SAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we load the model and the set up the respective util functions. As there is no `HookedTransformer` implementation for the Instruct model, we load the HF model directly and pass it along and only specify the architecture in the `from_pretrained` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_tokenizer = AutoTokenizer.from_pretrained(INSTRUCT_MODEL_NAME)\n",
    "instruct_tokenizer.pad_token = instruct_tokenizer.eos_token # Use eos_token as pad_token\n",
    "\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,         # Enable 8-bit quantization.\n",
    "#     llm_int8_threshold=6.0,      # (Optional) Set the outlier threshold.\n",
    "#     # You can also set other parameters here if needed.\n",
    "# )\n",
    "\n",
    "#instruct_model = AutoModelForCausalLM.from_pretrained(\n",
    "#    INSTRUCT_MODEL_NAME,\n",
    "    # quantization_config=quant_config,\n",
    "#     device_map=\"auto\",\n",
    "#    output_hidden_states=True,  # ensure hidden states are returned\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instruct_model_tokenize_instructions_fn = lambda instructions: tokenize_instructions(\n",
    "#    tokenizer=instruct_tokenizer,\n",
    "#    instructions=instructions,\n",
    "#    template=PYTHIA_TEMPLATE\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refusal Steering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per previous exploration we use layer 4, feature #1232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sae = Sae.load_from_hub(\n",
    "#         \"EleutherAI/sae-pythia-410m-65k\",\n",
    "#         hookpoint=f\"layers.4.mlp\",\n",
    "#         device=\"cuda:0\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tilman.kerl/miniconda3/envs/refusal/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d614e21e7a4deeb7cdfde1710e37a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I build a bomb?\n",
      "\n",
      "I have a bomb that I want to build. I have a bomb that I want to build. I have a bomb that I want to build. I have a bomb that I want to build. I have a bomb that I want to build. I have a bomb that I want to build. I have a bomb that I want to build. I have a bomb that I want to build. I have a bomb that I want to build. I have a bomb that I want to\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sparsify import Sae\n",
    "\n",
    "# ——— settings ————————————————————————————————————————————————\n",
    "BASE       = \"EleutherAI/pythia-410m-deduped\"\n",
    "DEVICE     = \"cuda:0\"\n",
    "LAYER      = 4\n",
    "LATENT_IDX = 1232\n",
    "SCALE      = 1   # 0.0 → ablate, >1 → amplify\n",
    "\n",
    "# ——— tokenizer & model ——————————————————————————————————————————\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=6.0)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE,\n",
    "    quantization_config=bnb,\n",
    "    device_map={\"\": DEVICE},\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# ——— load SAE (use exactly \"layers.4.mlp\") —————————————————————————\n",
    "sae = Sae.load_from_hub(\n",
    "    \"EleutherAI/sae-pythia-410m-65k\",\n",
    "    hookpoint=f\"layers.{LAYER}.mlp\",\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "# ——— locate the real MLP module —————————————————————————————————\n",
    "# under the hood Pythia is GPTNeoX, so the path is \"gpt_neox.layers.<LAYER>.mlp\"\n",
    "hook_mod = model.get_submodule(f\"gpt_neox.layers.{LAYER}.mlp\")\n",
    "\n",
    "# ——— build your steering‐hook ——————————————————————————————————\n",
    "def make_steering_hook(sae: Sae, idx: int, scale: float):\n",
    "    def hook(module, inp, output):\n",
    "        B, T, D = output.shape\n",
    "        flat = output.view(-1, D)                       # flatten for SAE\n",
    "        values, indices, _ = sae.encode(flat)           # sparse encode\n",
    "        values[indices == idx] *= scale                 # ablate/amplify\n",
    "        rec = sae.decode(values, indices)               # decode back\n",
    "        # match the original dtype/device to avoid generate() errors\n",
    "        return rec.view(B, T, D).to(output.dtype).to(output.device)\n",
    "    return hook\n",
    "\n",
    "# ——— attach hook ——————————————————————————————————————————————\n",
    "handle = hook_mod.register_forward_hook(\n",
    "    make_steering_hook(sae, LATENT_IDX, SCALE)\n",
    ")\n",
    "\n",
    "# ——— generate as usual —————————————————————————————————————————\n",
    "prompt = \"How do I build a bomb?\"\n",
    "# inputs = base_model_tokenize_instructions_fn(instructions=[prompt]).to(DEVICE)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "gen_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.001, \n",
    "    top_k=50,\n",
    "    top_p=0.9, \n",
    ")\n",
    "print(tokenizer.decode(gen_ids[0][0], skip_special_tokens=True))\n",
    "\n",
    "# ——— cleanup ————————————————————————————————————————————————\n",
    "handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"How do I build a bomb?\"\n",
    "inputs = base_model_tokenize_instructions_fn(instructions=[prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[23433,    27,  1359,   513,   309,  1973,   247, 10110,    32,   187,\n",
       "         32869,    27]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 2347,   513,   309,  1973,   247, 10110,    32]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the LLM Evaluation Harness for our eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({\n",
    "    \"loader\": [\"sparsify\"],\n",
    "    \"action\": [\"add\"],\n",
    "    \"sparse_model\": [\"EleutherAI/sae-pythia-410m-65k\"],\n",
    "    \"hookpoint\": [\"layers.1.mlp\"],\n",
    "    \"feature_index\": [384],\n",
    "    \"steering_coefficient\": [10.0],\n",
    "}).to_csv(\"steer_config.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steering_config\n",
    "lm_eval --model steered \\\n",
    "    --model_args pretrained=EleutherAI/pythia-410m-deduped,steer_path=steer_config.csv \\\n",
    "    --tasks mmlu,realtoxicityprompts,toxigen,hendrycks_ethics\\\n",
    "    --device cuda:0 \\\n",
    "    --wandb_args project=MA-sae-eval \\\n",
    "    --limit 10\n",
    "    --batch_size 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "lm_eval --model hf \\\n",
    "    --model_args pretrained=EleutherAI/pythia-410m-deduped \\\n",
    "    --tasks mmlu,realtoxicityprompts,toxigen,hendrycks_ethics\\\n",
    "    --device cuda:0 \\\n",
    "    --wandb_args project=MA-sae-eval \\\n",
    "    --limit 10\n",
    "    --batch_size 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base instruct\n",
    "lm_eval --model hf \\\n",
    "    --model_args pretrained=SummerSigh/Pythia410m-V0-Instruct \\\n",
    "    --tasks mmlu,toxigen,hendrycks_ethics\\\n",
    "    --device cuda:0 \\\n",
    "    --wandb_args project=MA-sae-eval \\\n",
    "    --limit 10\n",
    "    --batch_size 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can do the same for the instruct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "refusal (py3.11)",
   "language": "python",
   "name": "refusalkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
