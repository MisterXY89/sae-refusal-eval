{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steer Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch pandas numpy scikit-learn matplotlib seaborn tqdm sae-lens --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tilmankerl/Documents/UNI/MA/ma_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-70m-deduped\",\n",
    "  revision=\"step3000\",\n",
    "  cache_dir=\"./pythia-70m-deduped/step3000\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  \"EleutherAI/pythia-70m-deduped\",\n",
    "  revision=\"step3000\",\n",
    "  cache_dir=\"./pythia-70m-deduped/step3000\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/Users/tilmankerl/Documents/UNI/MA/ma_env/lib/python3.11/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"The homework is difficult. I need help.\", return_tensors=\"pt\")\n",
    "tokens = model.generate(**inputs)\n",
    "base_answer = tokenizer.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"pythia-70m-deduped-att-sm\", \n",
    "    sae_id=\"blocks.1.hook_attn_out\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architecture': 'standard',\n",
       " 'd_in': 512,\n",
       " 'd_sae': 32768,\n",
       " 'dtype': 'torch.float32',\n",
       " 'device': 'cpu',\n",
       " 'model_name': 'pythia-70m-deduped',\n",
       " 'hook_name': 'blocks.1.hook_attn_out',\n",
       " 'hook_layer': 1,\n",
       " 'hook_head_index': None,\n",
       " 'activation_fn_str': 'relu',\n",
       " 'activation_fn_kwargs': {},\n",
       " 'apply_b_dec_to_input': True,\n",
       " 'finetuning_scaling_factor': False,\n",
       " 'sae_lens_training_version': None,\n",
       " 'prepend_bos': False,\n",
       " 'dataset_path': 'EleutherAI/the_pile_deduplicated',\n",
       " 'dataset_trust_remote_code': True,\n",
       " 'context_size': 128,\n",
       " 'normalize_activations': 'none',\n",
       " 'neuronpedia_id': 'pythia-70m-deduped/1-att-sm'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAE(\n",
       "  (activation_fn): ReLU()\n",
       "  (hook_sae_input): HookPoint()\n",
       "  (hook_sae_acts_pre): HookPoint()\n",
       "  (hook_sae_acts_post): HookPoint()\n",
       "  (hook_sae_output): HookPoint()\n",
       "  (hook_sae_recons): HookPoint()\n",
       "  (hook_sae_error): HookPoint()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10., -10., -10.,  ..., -10., -10., -10.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXSdpaAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from: https://www.neuronpedia.org/gemma-2-9b-it/steer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to steer a model. Here, we do the following for each feature being steered:\n",
    "\n",
    "Multiply the steering strength by the strength multiple to get the steering coefficient.\n",
    "Get the steering vector from the SAE, which is the feature's decoder weights.\n",
    "Add the steering coefficient * steering vector to the activations.\n",
    "\n",
    "In code, it looks like this:\n",
    "\n",
    "    steering_coefficient = strength_multiple * steering_strength\n",
    "    steering_vector = sae.W_dec[feature_index]\n",
    "    activations += steering_coefficient * steering_vector\n",
    "    \n",
    "In the method we use, a strength_multiple of 0 means no steering will occur.\n",
    "Another variation of steering multiplies the steering vector by the top known activation value as well. That's a totally valid method, but here's why this version doesn't do this (but we may add it in the future):\n",
    "\n",
    "We may be missing activations for features that are very sparse (or we may not have run enough test prompts during dashboard generation) - and we'd still want to allow steering for those features.\n",
    "We want results to be consistent, regardless of what activations are known - eg if someone else steers the same feature with the same method (but have different top activations), we want results to be same, or very close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature used for steering (used neuropedia to find the feature)\n",
    "feature_index = 15698\n",
    "\n",
    "# scale & intensity\n",
    "steering_strength = 5.0 \n",
    "strength_multiple = 1.2  \n",
    "\n",
    "steering_coefficient = strength_multiple * steering_strength\n",
    "# decoder weights for the feature\n",
    "steering_vector = sae.W_dec[feature_index]  \n",
    "\n",
    "# Define a hook to modify the activations\n",
    "def steer_activations(module, inputs, output):\n",
    "    # instance check to avoid tuple error\n",
    "    if isinstance(output, tuple):\n",
    "        output_tensor = output[0]  \n",
    "        modified_output = output_tensor + (steering_coefficient * steering_vector)\n",
    "        return (modified_output,) + output[1:]  \n",
    "    \n",
    "    return output + (steering_coefficient * steering_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steering_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['', 'gpt_neox', 'gpt_neox.embed_in', 'gpt_neox.emb_dropout', 'gpt_neox.layers', 'gpt_neox.layers.0', 'gpt_neox.layers.0.input_layernorm', 'gpt_neox.layers.0.post_attention_layernorm', 'gpt_neox.layers.0.post_attention_dropout', 'gpt_neox.layers.0.post_mlp_dropout', 'gpt_neox.layers.0.attention', 'gpt_neox.layers.0.attention.rotary_emb', 'gpt_neox.layers.0.attention.query_key_value', 'gpt_neox.layers.0.attention.dense', 'gpt_neox.layers.0.attention.attention_dropout', 'gpt_neox.layers.0.mlp', 'gpt_neox.layers.0.mlp.dense_h_to_4h', 'gpt_neox.layers.0.mlp.dense_4h_to_h', 'gpt_neox.layers.0.mlp.act', 'gpt_neox.layers.1', 'gpt_neox.layers.1.input_layernorm', 'gpt_neox.layers.1.post_attention_layernorm', 'gpt_neox.layers.1.post_attention_dropout', 'gpt_neox.layers.1.post_mlp_dropout', 'gpt_neox.layers.1.attention', 'gpt_neox.layers.1.attention.rotary_emb', 'gpt_neox.layers.1.attention.query_key_value', 'gpt_neox.layers.1.attention.dense', 'gpt_neox.layers.1.attention.attention_dropout', 'gpt_neox.layers.1.mlp', 'gpt_neox.layers.1.mlp.dense_h_to_4h', 'gpt_neox.layers.1.mlp.dense_4h_to_h', 'gpt_neox.layers.1.mlp.act', 'gpt_neox.layers.2', 'gpt_neox.layers.2.input_layernorm', 'gpt_neox.layers.2.post_attention_layernorm', 'gpt_neox.layers.2.post_attention_dropout', 'gpt_neox.layers.2.post_mlp_dropout', 'gpt_neox.layers.2.attention', 'gpt_neox.layers.2.attention.rotary_emb', 'gpt_neox.layers.2.attention.query_key_value', 'gpt_neox.layers.2.attention.dense', 'gpt_neox.layers.2.attention.attention_dropout', 'gpt_neox.layers.2.mlp', 'gpt_neox.layers.2.mlp.dense_h_to_4h', 'gpt_neox.layers.2.mlp.dense_4h_to_h', 'gpt_neox.layers.2.mlp.act', 'gpt_neox.layers.3', 'gpt_neox.layers.3.input_layernorm', 'gpt_neox.layers.3.post_attention_layernorm', 'gpt_neox.layers.3.post_attention_dropout', 'gpt_neox.layers.3.post_mlp_dropout', 'gpt_neox.layers.3.attention', 'gpt_neox.layers.3.attention.rotary_emb', 'gpt_neox.layers.3.attention.query_key_value', 'gpt_neox.layers.3.attention.dense', 'gpt_neox.layers.3.attention.attention_dropout', 'gpt_neox.layers.3.mlp', 'gpt_neox.layers.3.mlp.dense_h_to_4h', 'gpt_neox.layers.3.mlp.dense_4h_to_h', 'gpt_neox.layers.3.mlp.act', 'gpt_neox.layers.4', 'gpt_neox.layers.4.input_layernorm', 'gpt_neox.layers.4.post_attention_layernorm', 'gpt_neox.layers.4.post_attention_dropout', 'gpt_neox.layers.4.post_mlp_dropout', 'gpt_neox.layers.4.attention', 'gpt_neox.layers.4.attention.rotary_emb', 'gpt_neox.layers.4.attention.query_key_value', 'gpt_neox.layers.4.attention.dense', 'gpt_neox.layers.4.attention.attention_dropout', 'gpt_neox.layers.4.mlp', 'gpt_neox.layers.4.mlp.dense_h_to_4h', 'gpt_neox.layers.4.mlp.dense_4h_to_h', 'gpt_neox.layers.4.mlp.act', 'gpt_neox.layers.5', 'gpt_neox.layers.5.input_layernorm', 'gpt_neox.layers.5.post_attention_layernorm', 'gpt_neox.layers.5.post_attention_dropout', 'gpt_neox.layers.5.post_mlp_dropout', 'gpt_neox.layers.5.attention', 'gpt_neox.layers.5.attention.rotary_emb', 'gpt_neox.layers.5.attention.query_key_value', 'gpt_neox.layers.5.attention.dense', 'gpt_neox.layers.5.attention.attention_dropout', 'gpt_neox.layers.5.mlp', 'gpt_neox.layers.5.mlp.dense_h_to_4h', 'gpt_neox.layers.5.mlp.dense_4h_to_h', 'gpt_neox.layers.5.mlp.act', 'gpt_neox.final_layer_norm', 'gpt_neox.rotary_emb', 'embed_out'])\n"
     ]
    }
   ],
   "source": [
    "submodule_names = dict(model.named_modules()).keys()\n",
    "print(submodule_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x12f769390>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register the hook \n",
    "# --> means that the steering function will be called every time the module \"gpt_neox.layers.1.attention\" is called\n",
    "# \"blocks.1.hook_attn_out\"  => \"gpt_neox.layers.1.attention\"\n",
    "hook_name = \"gpt_neox.layers.1.attention\"\n",
    "hook = model.get_submodule(hook_name)\n",
    "hook.register_forward_hook(lambda module, inputs, output: steer_activations(module, inputs, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Run inference with steering applied\n",
    "prompt = \"The homework is difficult. I need help.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Base answer: The homework is difficult. I need help.\n",
      "\n",
      "I have a problem with the problem. I\n",
      "Steered answer: The homework is difficult. I need help. I need to get the best of my homework.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decode and print the output\n",
    "steered_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\" Base answer: {base_answer}\")\n",
    "print(f\"Steered answer: {steered_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
