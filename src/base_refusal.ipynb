{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refusal Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to reproduce the findings of \"Base Models Refuse Too\" for the Pythia models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the necessary libraries once, then comment out the installation cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torch pandas numpy scikit-learn matplotlib seaborn tqdm sae-lens transformer-lens jaxtyping einops colorama accelerate bitsandbytes>0.37.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tilmankerl/Documents/UNI/MA/ma_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import functools\n",
    "from colorama import Fore, Style\n",
    "import textwrap\n",
    "from jaxtyping import Float\n",
    "import einops\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformer_lens\n",
    "from sae_lens import SAE\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "\n",
    "from jaxtyping import Int\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of our own (util) functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_tools.instructions import get_harmful_instructions, get_harmless_instructions\n",
    "from utils.templates import PYTHIA_TEMPLATE\n",
    "from utils.generation import ( \n",
    "    get_generations, format_instruction, tokenize_instructions, \n",
    "    act_add_hook, direction_ablation_hook\n",
    ")\n",
    "from utils.refusal import (\n",
    "    get_refusal_scores, extract_refusal_direction,\n",
    "    get_refusal_direction_hooks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"pythia-410m\": {\n",
    "        \"base_model\": {},\n",
    "        \"instruct_model\": {},\n",
    "        \"hooked_base_model\": {},\n",
    "        \"hooked_instruct_model\": {}\n",
    "    }\n",
    "}\n",
    "\n",
    "BASE_MODEL_NAME = \"EleutherAI/pythia-410m-deduped\"\n",
    "INSTRUCT_MODEL_NAME = \"SummerSigh/Pythia410m-V0-Instruct\"\n",
    "\n",
    "STEERING_COEFF = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the data and the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmless_inst_train, harmless_inst_test = get_harmless_instructions()\n",
    "harmful_inst_train, harmful_inst_test = get_harmful_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we define a function to return model, and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = HookedTransformer.from_pretrained(\n",
    "    # \"qwen1.5-0.5b-chat\",\n",
    "    # '\"SummerSigh/Pythia410m-V0-Instruct\",\n",
    "    # \"EleutherAI/pythia-410m\",    \n",
    "    BASE_MODEL_NAME\n",
    "    default_padding_side='left',\n",
    "\n",
    ")\n",
    "base_model.tokenizer.padding_side = 'left'\n",
    "base_model.tokenizer.add_special_tokens({'pad_token': '<|padding|>'})\n",
    "\n",
    "# this is the layer where we'll intervene?\n",
    "base_model_layer = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up our tokenize and generation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_tokenize_instructions_fn = functools.partial(tokenize_instructions, tokenizer=base_model.tokenizer, template=PYTHIA_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_baseline_generations = get_generations(\n",
    "    base_model, harmful_inst_test, base_model_tokenize_instructions_fn, \n",
    "    fwd_hooks=[], max_tokens_generated=100, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can inspect the generations and look at the refusal rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_baseline_generations[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_refusal = get_refusal_scores(base_model_baseline_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_model_refusal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we store these results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"pythia-410m\"][\"base_model\"] = {    \n",
    "    \"generations\": base_model_baseline_generations,\n",
    "    \"refusal\": base_model_refusal,\n",
    "    \"name\": BASE_MODEL_NAME\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruct Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we load the model and the set up the respective util functions. As there is now `HookedTransformer` implementation for the Instruct model, we load the HF model directly and pass it along and only specify the architecture in the `from_pretrained` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model_hf = AutoModelForCausalLM.from_pretrained(INSTRUCT_MODEL_NAME)\n",
    "\n",
    "instruct_model = HookedTransformer.from_pretrained(\n",
    "    \"EleutherAI/pythia-410m-deduped\",\n",
    "    hf_model=instruct_model_hf,\n",
    "    default_padding_side='left',\n",
    "  )\n",
    "\n",
    "instruct_tokenizer = AutoTokenizer.from_pretrained(INSTRUCT_MODEL_NAME)\n",
    "instruct_tokenizer.padding_side = 'left'\n",
    "instruct_tokenizer.pad_token = instruct_tokenizer.eos_token\n",
    "\n",
    "# chat_model.tokenizer.add_special_tokens({'pad_token': '<|padding|>'})\n",
    "\n",
    "instruct_model_layer = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model_tokenize_instructions_fn = functools.partial(tokenize_instructions, tokenizer=instruct_tokenizer, template=PYTHIA_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_baseline_generations = get_generations(\n",
    "    instruct_model, harmful_inst_test, instruct_model_tokenize_instructions_fn, \n",
    "    fwd_hooks=[], max_tokens_generated=100, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can inspect the generations and look at the refusal rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_baseline_generations[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model_refusals = get_refusal_scores(instruct_baseline_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(instruct_model_refusals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"pythia-410m\"][\"instruct_model\"] = {    \n",
    "    \"generations\": base_model_baseline_generations,\n",
    "    \"refusal\": base_model_refusal,\n",
    "    \"name\": INSTRUCT_MODEL_NAME\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refusal Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract the refusal direction from both models, following the \"Base Models Refuse Too\" blog post + Arditi et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_refusal_direction = extract_refusal_direction(\n",
    "    model=base_model, \n",
    "    model_name=BASE_MODEL_NAME,\n",
    "    train=harmful_inst_train,\n",
    "    n_inst_train=len(harmful_inst_train),\n",
    "    layer=base_model_layer,\n",
    "    # what is this?\n",
    "    pos=0,\n",
    "    pythia_template=PYTHIA_TEMPLATE,\n",
    "    tokenize_instructions=base_model_tokenize_instructions_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_refusal_hook = get_refusal_direction_hooks(\n",
    "    model=base_model,\n",
    "    refusal_dir=base_model_refusal_direction,\n",
    "    act_add_hook=act_add_hook, \n",
    "    direction_ablation_hook=direction_ablation_hook,\n",
    "    steering_coeff=STEERING_COEFF,\n",
    "    layer=base_model_layer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_intervention_generations = get_generations(\n",
    "    base_model, \n",
    "    instructions= harmful_inst_test, \n",
    "    tokenize_instructions_fn=base_model_tokenize_instructions_fn,\n",
    "    fwd_hooks=base_refusal_hook,\n",
    "    max_tokens_generated=100, \n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_intervention_refusals = get_refusal_scores(base_intervention_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"pythia-410m\"][\"hooked_base_model\"] = {\n",
    "    \"generations\": base_intervention_generations,\n",
    "    \"refusal\": base_model_intervention_refusals,\n",
    "    \"name\": BASE_MODEL_NAME\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
